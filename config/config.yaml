dataset:
  name: &dataset 'webqsp'
  root: ./datasets
  emb_name: 'gte-large-en-v1.5'
  coarse_filter: True
  coarse_num_or_ratio: 300
  skip_no_path: True

env:
  seed: 42
  device: &device 'cuda:0'

llms:
  device: *device
  llm_frozen: False
  llm_model_path: meta-llama/Llama-3.2-1B-Instruct
  llm_model_name: &llm 'Llama-3.2-1B-Instruct'

# TODO: hyper-param from G-Retriever
train:
  warmup:
    lr: 0.001
    wd: 0.
    num_epochs: 100
    patience: 10
  lr_ret: 5e-4
  wd_ret: 0.
  lr_llm: 1e-5
  wd_llm: 0.05
  num_epochs: 30
  patience: 6
  batch_size: 2
  grad_steps: 5
  llm_frozen_epoch: 5
  
# graphsage
retriever:
  gnn:
    model_type: &gnn PNA
    topic_pe: False
    hidden_size: 1024
    num_layers: 4
    aggr: 'mean'
    learn_non_text: False

algorithm:
  filtering: topk # "topk" or "idp-bern"
  filtering_num_or_ratio: 0.15 # int or float
  triplet2text: drop # "drop" or "mask"
  grad_normalize: True
  gumbel: True
  warmup: False
  warmup_epochs: 15


logging:
  root: ./logging
  dataset: *dataset
  llm: *llm
  ret: *gnn

#    # Inference
#    parser.add_argument("--eval_batch_size", type=int, default=16)
#
#    # LLM related
#    parser.add_argument("--llm_num_virtual_tokens", type=int, default=10)
#    parser.add_argument("--output_dir", type=str, default='output')
#    parser.add_argument("--max_txt_len", type=int, default=512)
#    parser.add_argument("--max_new_tokens", type=int, default=32)
#    parser.add_argument("--max_memory", type=csv_list, default=[80,80])
#
#    # GNN related
#    parser.add_argument("--gnn_in_dim", type=int, default=1024)
#    parser.add_argument("--gnn_hidden_dim", type=int, default=1024)
#    parser.add_argument("--gnn_num_heads", type=int, default=4)
#    parser.add_argument("--gnn_dropout", type=float, default=0.0)