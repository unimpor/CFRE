dataset:
  name: &dataset 'cwq'
  root: ./datasets
  emb_name: 'gte-large-en-v1.5'
  post_filter: "metrics_rev_scored_100.pth"
  skip_no_path: True

env:
  seed: 42
  device: &device 'cuda:0'

# Llama-3.2-1B-Instruct
llms:
  device: *device
  llm_frozen: False  # deprecated
  # llm_model_path: meta-llama/Llama-3.2-1B-Instruct
  llm_model_name_or_path: &llm 'meta-llama/Meta-Llama-3.1-8B-Instruct' # optional: gpt-4o-mini
  tensor_parallel_size: 1
  max_seq_len_to_capture: 16384 
  max_tokens: 4000 
  seed: 0 
  temperature: 0
  frequency_penalty: 0.14
  prompt_mode: sys_icl_dc_reverse
  
train:
  warmup:
    lr: 0.001
    wd: 0.
    num_epochs: 100
    patience: 10
  lr_ret: 1e-3
  wd_ret: 0.
  lr_llm: 1e-5  # deprecated
  wd_llm: 0.05  # deprecated
  num_epochs: 50
  patience: 6
  batch_size: 16
  grad_steps: 5 # deprecated
  llm_frozen_epoch: 5 # deprecated
  
# graphsage
retriever:
  model_type: &gnn DDE
  topic_pe: True
  hidden_size: 1024
  learn_non_text: False
  DDE:
    num_rounds: 2
    num_reverse_rounds: 2

algorithm:
  filtering: topk # "topk" or "idp-bern"
  filtering_num_or_ratio: 100 # int or float
  triplet2text: drop # "drop" or "mask"
  # grad_normalize: True  # deprecated
  # gumbel: True # deprecated
  # warmup: False # deprecated
  # warmup_epochs: 15 # deprecated
  coeff1: 0.1
  coeff2: 0.1
  reward_metrics: "recall"
  perturb_per_sample: 1
  regularize: "wp"
  set_moving_baseline: False
  tau: 2.0
  # constant_ratio: 0.3 # deprecated
  gumbel_strength: 1.0
  baseline_order_invariant: True


logging:
  root: ./logging
  dataset: *dataset
  llm: *llm
  ret: *gnn

