dataset:
  name: 'webqsp'
  root: ./datasets
  emb_name: 'gte-large-en-v1.5'
  coarse_filter: True
  coarse_num_or_ratio: 300
  skip_no_path: True

env:
  seed: 42
  device: &device 'cuda:0'

llms:
  device: *device
  llm_frozen: False
  llm_model_path: meta-llama/Llama-3.2-1B-Instruct
  llm_model_name: Llama-3.2-1B-Instruct

# TODO: hyper-param from G-Retriever
train:
  lr: 1e-5
  wd: 0.05
  patience: 2
  batch_size: 2
  grad_steps: 2
  num_epochs: 100
  

log:
  root: <TODO>
  model_save: <TODO>


retriever:
  gnn:
    model_type: graphsage
    topic_pe: False
    hidden_size: 1024
    num_layers: 5
    aggr: 'mean'
    learn_non_text: False

algorithm:
  filtering: topk # "topk" or "idp-bern"
  filtering_num_or_ratio: 50 # int or float
  triplet2text: drop # "drop" or "mask"
  grad_normalize: True
  warmup: True
  warmup_epochs: 5

#    # Inference
#    parser.add_argument("--eval_batch_size", type=int, default=16)
#
#    # LLM related
#    parser.add_argument("--llm_num_virtual_tokens", type=int, default=10)
#    parser.add_argument("--output_dir", type=str, default='output')
#    parser.add_argument("--max_txt_len", type=int, default=512)
#    parser.add_argument("--max_new_tokens", type=int, default=32)
#    parser.add_argument("--max_memory", type=csv_list, default=[80,80])
#
#    # GNN related
#    parser.add_argument("--gnn_in_dim", type=int, default=1024)
#    parser.add_argument("--gnn_hidden_dim", type=int, default=1024)
#    parser.add_argument("--gnn_num_heads", type=int, default=4)
#    parser.add_argument("--gnn_dropout", type=float, default=0.0)