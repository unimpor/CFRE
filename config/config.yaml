dataset:
  name: 'cwq'
  root: <TODO>
  emb_name: 'gte-large-en-v1.5'
  coarse_filter: True
  coarse_num: 500

llms:
  llm_frozen: True
  llm_model_path: <TODO>
  llm_model_name: <TODO>

# TODO: hyper-param from G-Retriever
train:
  seed: 42
  lr: 1e-5
  wd: 0.05
  patience: 2
  batch_size: 8
  grad_steps: 2
  num_epochs: 10
  warmup_epochs: 2

retriever:
  gnn:
    model_type: graphsage

algorithm:
  sampling: topk # "topk" or "idp-bern"
  triplet2text: drop # "drop" or "mask"

#    # Inference
#    parser.add_argument("--eval_batch_size", type=int, default=16)
#
#    # LLM related
#    parser.add_argument("--llm_num_virtual_tokens", type=int, default=10)
#    parser.add_argument("--output_dir", type=str, default='output')
#    parser.add_argument("--max_txt_len", type=int, default=512)
#    parser.add_argument("--max_new_tokens", type=int, default=32)
#    parser.add_argument("--max_memory", type=csv_list, default=[80,80])
#
#    # GNN related
#    parser.add_argument("--gnn_model_name", type=str, default='gt')
#    parser.add_argument("--gnn_num_layers", type=int, default=4)
#    parser.add_argument("--gnn_in_dim", type=int, default=1024)
#    parser.add_argument("--gnn_hidden_dim", type=int, default=1024)
#    parser.add_argument("--gnn_num_heads", type=int, default=4)
#    parser.add_argument("--gnn_dropout", type=float, default=0.0)